{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torchtext.transforms as T\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def loadFiles(file):\n",
    "    with open(file, \"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    print(\"The size of the dataset is:\", len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "def separateData(data):\n",
    "    X = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset is: 10000\n"
     ]
    }
   ],
   "source": [
    "data = loadFiles(r'./english-german-both.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng, germ = separateData(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLongestSequence(sentList):\n",
    "    return max(len(seq.split()) for seq in sentList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scotty/.local/lib/python3.10/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(\n",
      "/home/scotty/.local/lib/python3.10/site-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = get_tokenizer('spacy', language='en')\n",
    "deTokenizer = get_tokenizer('spacy', language='de')\n",
    "def yieldTokensEn(data):\n",
    "    for text in data:\n",
    "        yield enTokenizer(text[:-1])\n",
    "\n",
    "def yieldTokensDe(data):\n",
    "    \n",
    "    for text in data:\n",
    "        yield deTokenizer(text[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabEn = build_vocab_from_iterator(yieldTokensEn(eng), specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"], special_first=True)\n",
    "vocabDe = build_vocab_from_iterator(yieldTokensDe(germ), specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"], special_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textPipelineEn = lambda x: vocabEn(enTokenizer(x))\n",
    "textPipelineDe = lambda x: vocabDe(deTokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "trainEn, testEn, trainDe, testDe = train_test_split(eng, germ, test_size=0.1, random_state=SEED)\n",
    "trainEn, valEn, trainDe, valDe = train_test_split(eng, germ, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "PAD_IDX = vocabEn(['<pad>'])[0]\n",
    "SOS_IDX = vocabEn(['<sos>'])\n",
    "EOS_IDX = vocabEn(['<eos>'])\n",
    "print(PAD_IDX)\n",
    "print(SOS_IDX)\n",
    "print(EOS_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def generateData(eng, deu):\n",
    "    data = []\n",
    "    for en, de in zip(eng, deu):\n",
    "        enTensor = torch.tensor(textPipelineEn(en[:-1]), dtype=torch.long)\n",
    "        deTensor = torch.tensor(textPipelineDe(de[:-1]), dtype=torch.long)\n",
    "        data.append((enTensor, deTensor))\n",
    "    return data\n",
    "\n",
    "\n",
    "trainData = generateData(trainEn, trainDe)\n",
    "valData = generateData(valEn, valDe)         \n",
    "testData = generateData(testEn, testDe)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "    \n",
    "def generateBatch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  # print(data_batch)\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor(SOS_IDX), de_item, torch.tensor(EOS_IDX)], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor(SOS_IDX), en_item, torch.tensor(EOS_IDX)], dim=0))\n",
    "  deLength = len(de_batch)\n",
    "  batch = pad_sequence(en_batch + de_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "  en_batch, de_batch = batch[:deLength], batch[deLength:]\n",
    "  return de_batch, en_batch\n",
    "\n",
    "\n",
    "trainIter = DataLoader(trainData, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generateBatch)\n",
    "valIter =  DataLoader(valData, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generateBatch)\n",
    "testIter = DataLoader(testData, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generateBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The english has 2594 words.\n",
      "The german has 4167 words.\n"
     ]
    }
   ],
   "source": [
    "from model.transformer import Transformer\n",
    "from model.positenc import PositionalEncodingTorch\n",
    "from torch.nn import Embedding, Module, Linear\n",
    "\n",
    "EMB_DIM = 64\n",
    "HEADS = 8\n",
    "LINEAR_DIM = 2048\n",
    "DROPOUT = 0.1\n",
    "LAYERS = 6\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.98\n",
    "EPSILON = 10**-9\n",
    "ENG_VOCAB_LEN = vocabEn.__len__()\n",
    "DE_VOCAB_LEN = vocabDe.__len__()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"The english has {ENG_VOCAB_LEN} words.\")\n",
    "print(f\"The german has {DE_VOCAB_LEN} words.\")\n",
    "\n",
    "EN_MAX_SEQ_LEN, DE_MAX_SEQ_LEN = findLongestSequence(eng) + 2 , findLongestSequence(germ) + 2\n",
    "\n",
    "#mask for the decoder\n",
    "def createMask(x):\n",
    "    batch, seq_length, _ = x.size()\n",
    "    mask = torch.ones((batch, seq_length, seq_length)).to(device)\n",
    "    mask = torch.tril(mask, diagonal=0)\n",
    "    return mask\n",
    "\n",
    "def createMaskGen(x):\n",
    "    x = x.unsqueeze(1)\n",
    "    batch, _, seq_length = x.size()\n",
    "    mask = torch.ones((batch, 1, seq_length)).to(device)\n",
    "    maskR = torch.logical_and(mask, x)\n",
    "    maskC = torch.logical_and(x.transpose(-1, 1), mask.transpose(-1, 1))\n",
    "    mask = torch.multiply(maskR, maskC)\n",
    "    return mask\n",
    "    \n",
    "\n",
    "\n",
    "class TransformerModel(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embEn = Embedding(ENG_VOCAB_LEN, EMB_DIM)\n",
    "        self.embDe = Embedding(DE_VOCAB_LEN, EMB_DIM)\n",
    "        self.positEn = PositionalEncodingTorch(EN_MAX_SEQ_LEN, EMB_DIM)\n",
    "        self.positDe = PositionalEncodingTorch(DE_MAX_SEQ_LEN, EMB_DIM)\n",
    "        self.transformer = Transformer(LAYERS, EMB_DIM, EMB_DIM, HEADS, LINEAR_DIM, DROPOUT)\n",
    "        self.linear = Linear(EMB_DIM, DE_VOCAB_LEN)\n",
    "        \n",
    "    def forward(self, eng, de, device=device):\n",
    "        mask1 = createMaskGen(eng)\n",
    "        eng = self.embEn(eng)\n",
    "        eng = self.positEn(eng, device=device)\n",
    "        mask = createMaskGen(de)\n",
    "        de = self.embDe(de)\n",
    "        de = self.positDe(de, device=device)\n",
    "        mask2 = createMask(de)\n",
    "        mask2 = torch.logical_and(mask, mask2)\n",
    "        dec = self.transformer(eng, de, mask1, mask2)\n",
    "        lin = self.linear(dec)    \n",
    "        return lin\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLRScheduler(object):\n",
    "    def __init__(self,  warmup_steps:int=10, d_model:int=512):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    \n",
    "    def __call__(self, epoch):\n",
    "        epoch = epoch + 1\n",
    "        minimum = min(epoch**-0.5, epoch * ((self.warmup_steps) ** (-1.5)))\n",
    "        return (self.d_model**-0.5 ) * minimum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(prediction, target):\n",
    "    padding_mask = torch.logical_not(torch.eq(target, torch.tensor(0)))\n",
    "    accuracy = torch.eq(target, torch.argmax(prediction, axis=2))\n",
    "    accuracy = torch.logical_and(padding_mask, accuracy)\n",
    "    accuracy = accuracy.type(torch.float32)\n",
    "    padding_mask = padding_mask.type(torch.float32)\n",
    "    return torch.sum(accuracy) / torch.sum(padding_mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(model, testIter, loss, device):\n",
    "    lossPerBatch = []\n",
    "    accPerBatch = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, (X, y) in enumerate(testIter):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            out = model(X[:, 1:], y[:, :-1])\n",
    "            l = loss(out.contiguous().view(-1, 4167), y[:, 1:].contiguous().view(-1))\n",
    "            a = calculateAccuracy(out, y[:, 1:])\n",
    "            lossPerBatch.append(l.item())\n",
    "            accPerBatch.append(a.item())\n",
    "        meanLoss = sum(lossPerBatch)/len(lossPerBatch)\n",
    "    return lossPerBatch, meanLoss, accPerBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "model = TransformerModel()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), betas=(BETA_1, BETA_2), eps=EPSILON)\n",
    "loss = nn.CrossEntropyLoss(ignore_index=0)\n",
    "scheduler = TransformerLRScheduler(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "def train(model, trainIter, \n",
    "          testIter=None, \n",
    "          epochs=None, \n",
    "          loss=None, \n",
    "          optimizer=None, \n",
    "          device=device, \n",
    "          scheduler=None):\n",
    "    # model.to(device)\n",
    "    # pbar = trange(epochs, desc=\"Epochs \", unit=\"batches\")\n",
    "    # with tqdm(trainIter, unit=\"epochs\") as tepoch:\n",
    "    logs_dic = {\n",
    "        \"valildationLoss\": [],\n",
    "        \"trainingLoss\" : [],\n",
    "        \"validationAccuracy\": [],\n",
    "        \"trainingAccuracy\": []\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        trainLossPerBatch = []\n",
    "        trainAccuracyPerBatch = []\n",
    "        with tqdm(trainIter, unit=\"batches\") as tepoch:\n",
    "            for i, (X,y) in enumerate(tepoch):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                out = model(X[:, 1:], y[:, :-1])\n",
    "                l = loss(out.contiguous().view(-1, 4167), y[:, 1:].contiguous().view(-1))\n",
    "                acc = calculateAccuracy(out, y[:, 1:])\n",
    "                trainLossPerBatch.append(l.item())\n",
    "                trainAccuracyPerBatch.append(acc.item())\n",
    "                tepoch.set_description(f\"Epoch {epoch + 1}\")            \n",
    "                tepoch.set_postfix(loss=l.item(), accuracy=acc.item())\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "            valLoss, meanValLoss, valAcc = validateModel(model, testIter, loss=loss, device=device)\n",
    "            print(f\"The validation loss is: {meanValLoss}\")\n",
    "            logs_dic[\"valildationLoss\"].append(valLoss)\n",
    "            logs_dic[\"trainingLoss\"].append(trainLossPerBatch)\n",
    "            logs_dic[\"trainingAccuracy\"].append(trainAccuracyPerBatch)\n",
    "            logs_dic[\"validationAccuracy\"].append(valAcc)\n",
    "            # print(f\"Epoch: {epoch+1}     loss: {l}\")\n",
    "            if scheduler:\n",
    "                if scheduler.__module__ == lr_scheduler.__name__:\n",
    "                    scheduler.step()\n",
    "                else:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        lr = scheduler(epoch)\n",
    "                        param_group['lr'] = lr\n",
    "                \n",
    "    return logs_dic\n",
    "# history = train(model, trainIter, testIter=valIter, epochs=1, loss=loss, optimizer=optimizer, device=device, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveHistory(history, filename):\n",
    "    print(\"pickling history.\")\n",
    "    with open(filename, 'wb') as fp:\n",
    "        pickle.dump(history, fp)\n",
    "    print(\"successfully pickled\") \n",
    "    \n",
    "# saveHistory(history, \"./history\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# lr = TransformerLRScheduler(1000)\n",
    "# plt.plot([i for i in range(5000)], [lr(i) for i in range(5000)])\n",
    "# plt.grid()\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel(\"Learning Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.save_dict(), './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 7])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n",
      "prediction is:  torch.Size([4, 4167])\n",
      "tensor([2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test(model, encInput):\n",
    "    model.eval()\n",
    "    batch, seq_length = encInput.size()\n",
    "    decOutput =  torch.tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])  \n",
    "    decOutput = torch.tile(decOutput, (batch, 1)) \n",
    "    for i in range(DE_MAX_SEQ_LEN):\n",
    "        prediction = model(encInput, decOutput)\n",
    "        prediction = prediction[:, -1, :]\n",
    "        predicted_id = torch.argmax(prediction, dim=-1)\n",
    "        print(predicted_id)\n",
    "        \n",
    "        \n",
    "model.load_state_dict(torch.load('e150emb64b256d15l6.pth', map_location=device))\n",
    "for i, (X, y) in enumerate(testIter):\n",
    "    print(i, X.shape)\n",
    "    test(model, X)        \n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, encInput):\n",
    "    model.eval()\n",
    "    batch, seq_length = encInput.size()\n",
    "    out = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    decOutput =  torch.tensor(out)\n",
    "    decOutput = torch.tile(decOutput, (batch, 1))\n",
    "    for i in range(DE_MAX_SEQ_LEN-1):\n",
    "        prediction = model(encInput.to(device)[:, 1:], decOutput.to(device))\n",
    "        prediction = prediction[:, i, :]\n",
    "        predicted_id = torch.argmax(prediction, dim=-1)\n",
    "        decOutput[:, i+1] = predicted_id[ :]\n",
    "        print(predicted_id.tolist()[0])\n",
    "        if predicted_id.tolist()[0] == EOS_IDX[0]:\n",
    "          break\n",
    "    return decOutput\n",
    "testIter = DataLoader(testData, batch_size=1, shuffle=True, collate_fn=generateBatch)\n",
    "\n",
    "\n",
    "\n",
    "for i, (X, y) in enumerate(testIter):\n",
    "    pred = test(model, X)\n",
    "    for x, y, z in zip(pred.tolist(), X.tolist(), y.tolist()):\n",
    "      print(' '.join(vocabDe.lookup_tokens(x)))\n",
    "      print(' '.join(vocabEn.lookup_tokens(y)))\n",
    "      print(' '.join(vocabDe.lookup_tokens(z)))\n",
    "      print(\"\\n\")\n",
    "\n",
    "  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
