# Transformer implementation in pytorch

This is the application of [Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
Although during the training a simple dropout layer is added after the Positional Embeddings is added to the words embeddings. The training is just a small experiment of translating from english to german with only about 10k data points in the dataset. 
